{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afinamiento de modelos de lenguaje para dominios específicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Fine tuning de un modelo con dataset tipo 'instruct' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ajuste fino con datos tipo \"instruct\" adapta los modelos de lenguaje (LLMs) para responder eficazmente a instrucciones específicas.\n",
    "\n",
    "### **Características clave**:\n",
    "1. **Entrenamiento con pares de instrucciones y respuestas**: Se utilizan ejemplos como \"Resume este texto en dos frases\" y su respuesta esperada, para enseñar al modelo a seguir comandos.\n",
    "2. **Datos etiquetados o supervisados**: Las respuestas pueden ser creadas por humanos o generadas por modelos existentes y revisadas manualmente.\n",
    "\n",
    "### **Proceso**:\n",
    "1. **Creación del dataset**: Se compilan instrucciones y respuestas alineadas con los objetivos del usuario.\n",
    "2. **Ajuste fino supervisado (SFT)**: El modelo se entrena utilizando estos pares para mejorar su capacidad de responder a comandos de manera precisa y alineada con la intención del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes > /dev/null 2>&1\n",
    "!pip install datasets > /dev/null 2>&1\n",
    "!pip install peft > /dev/null 2>&1\n",
    "!pip install wandb > /dev/null 2>&1\n",
    "!pip install trl > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'nb01-instruct.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importar el modelo ya pre-entrenado y el tokenizador\n",
    "\n",
    "El modelo `unsloth/mistral-7b-instruct-v0.2-bnb-4bit` es una variante compacta del modelo Mistral de 7 mil millones de parámetros, diseñada para tareas de instrucción. Utiliza técnicas de cuantización en 4 bits (bnb-4bit) para optimizar el uso de memoria y permitir su ejecución en hardware menos potente sin perder demasiada precisión. Este modelo está ajustado para interpretar y responder instrucciones en lenguaje natural, siendo útil para aplicaciones en procesamiento de lenguaje natural (NLP) que requieren respuestas eficientes y precisas en un entorno optimizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\"\n",
    "tokenizer_name = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Carga de los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('json', data_files='/content/drive/MyDrive/training/data/list_of_strings.jsonl', split=\"train\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=64)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project='LLM_training',\n",
    "    name=model_name + '_autoregressive_instruct_fine_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuración del entrenamiento mediante LoRA y SFT\n",
    "\n",
    "- **LoRA (Low-Rank Adaptation):**  \n",
    "LoRA es una técnica utilizada para ajustar modelos grandes de lenguaje de manera eficiente. En lugar de actualizar todos los parámetros del modelo durante el entrenamiento, LoRA introduce matrices adicionales de bajo rango que capturan los cambios necesarios. Esto reduce significativamente el costo computacional y de memoria, permitiendo un ajuste fino (fine-tuning) eficiente sin necesidad de almacenar o modificar todos los parámetros originales del modelo.\n",
    "\n",
    "- **SFT (Supervised Fine-Tuning):**  \n",
    "SFT se refiere al ajuste fino de un modelo de lenguaje utilizando datos etiquetados de manera supervisada. Este proceso entrena el modelo para realizar tareas específicas mediante ejemplos claros de entrada y salida, como preguntas y respuestas, traducción, o clasificación. Es un paso clave para especializar modelos generales en tareas concretas o dominios específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']  # módulos objetivo para aplicar LoRA\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"no\",\n",
    "    eval_steps=10_000,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    max_seq_length=64,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=True,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./content/drive/MyDrive/training/models/fine_tuned_model_autoregressive\")\n",
    "tokenizer.save_pretrained(\"./content/drive/MyDrive/training/models/fine_tuned_model_autoregressive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./drive/MyDrive/training/models/fine_tuned_model_autoregressive/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar texto\n",
    "def generate_text(prompt, max_length=100, num_return_sequences=1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.01\n",
    "    )\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeros_a_palabras = {\n",
    "    1: \"uno\", 2: \"dos\", 3: \"tres\", 4: \"cuatro\", 5: \"cinco\",\n",
    "    6: \"seis\", 7: \"siete\", 8: \"ocho\", 9: \"nueve\"\n",
    "}\n",
    "\n",
    "texto_a_numero = {v: k for k, v in numeros_a_palabras.items()}\n",
    "\n",
    "def generar_serie_objetivo(objetivo):\n",
    "    serie = []\n",
    "    suma_actual = 0\n",
    "    while suma_actual < objetivo:\n",
    "        numero = random.randint(1, min(9, objetivo - suma_actual))\n",
    "        serie.append(numero)\n",
    "        suma_actual += numero\n",
    "    return serie\n",
    "\n",
    "\n",
    "def comprobar_suma(cadena):\n",
    "    numero_inicial = int(cadena.split()[0])\n",
    "\n",
    "    palabras = cadena.split()[3:]  # Ignorar \"es igual a\"\n",
    "    suma_numeros = sum(texto_a_numero.get(palabra, 0) for palabra in palabras)\n",
    "    return abs(numero_inicial - suma_numeros)\n",
    "\n",
    "def evaluar_modelo(n=10):\n",
    "    total_error = 0\n",
    "    for _ in range(n):\n",
    "        numero_aleatorio = random.randint(10, 50)\n",
    "        serie_aleatoria = generar_serie_objetivo(numero_aleatorio)\n",
    "        serie_en_palabras = \" más \".join(numeros_a_palabras[numero] for numero in serie_aleatoria)\n",
    "        prompt = f\"<s>[INST] {numero_aleatorio} es igual a [/INST]\"\n",
    "        \n",
    "        generated_texts = generate_text(prompt, max_length=45, num_return_sequences=1)\n",
    "        for text in generated_texts:\n",
    "            cadena = text.split(\"[INST]\")[1] if \"[INST]\" in text else text\n",
    "            error = comprobar_suma(cadena)\n",
    "            total_error += error\n",
    "    \n",
    "    return total_error / n\n",
    "\n",
    "mae = evaluar_modelo(100)\n",
    "print(f\"Error absoluto promedio (MAE): {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<s>[INST] 13 es igual a [/INST]\"\n",
    "generated_texts = generate_text(prompt, max_length=45, num_return_sequences=1)\n",
    "\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\\n{text}\\n\")\n",
    "    cadena = f\"{text}\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
